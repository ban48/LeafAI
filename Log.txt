LOG of important things

06/05
"PlantVillage" dataset -> good, but some relevant informations about species/diseases are missing. In few words, for some species only "healthy" or "ill" images existed.
Already divided in training and validation (used as testing) sets.

We decided to delete the species with only "ill" images (orange and squash in our case). The idea was: is better to unsuccessfully classify a disease rather than wrongly classifying.
BEFORE: 14 species - 21 diseases (including "healthy")
AFTER: 12 species - 20 diseases 

Later, for image normalization of our own dataset (in case is needed), we computed the mean and the standard deviation of our dataset. After we had computed it, we stored the results here:
true_mean = [ 0.47131167, 0.49435022, 0.42405355 ]
true_std = [ 0.17719288, 0.14827244, 0.19360321 ]



08/05
What's the best kernel size for the gaussian filter? We prrpared a portion of the jupyter paper in which we try different kernel sizes.
We decided to use a Kernel 3x3, looking at the different results made by different other kernels (5x5 or 7x7).

For the pre-processing part, we applied:
- Resize, obtaining a fixed size for every image (224, 224), so the image can be used for CNN
- Gaussian Blur, as mentioned, for every image in the dataset
- Augmentation, applying rotation, flips and HSV adjustments every time in a different way (random transformations)

IMPORTANT!!
These transformations (gaussian blur and augmentation) are applied EVERY TIME WE TRAIN the architecture, so "on-the-fly". This has different advantages, 
like less-storage usage and exploit different images every time(in an extreme point of view, is like using an "infinite" amount of data, since the images are slightly different every time). 



10/05
We decided to use a ResNet-18 for our first attempt of training/validation, making some changes from the original architecture:
- Last FC split: we put the original FC of the network equal to the identity network, then we created two different FC, one for species and the other for diseases.
- Loss: Cross-Entropy updated separetly, but unified (sum) for backpropagation (so weights update)
- Checkpoint storage: every epoch done during training/validation is stored into a specific file, keeping the last epoch done, in order to resume the training from the last 
    parameters computed in the i-th epoch
After only 10 epochs, with different devices (Mac Pro with 'mps' GPU and Windows notebook with Nvidia GPU), 
we obtained an interesting (and good) result: ~93% average accuracy for both species and disease. Other tests must be done with different architectures (CLIP, DINOv2) to compare results.



13/05
At the end the choices for the models are: ViT from torchvision, CLIP used with ResNet-50 and ViT, DINOv2.
We created also a file for every different model and a single training file for everyone of them, as a design choice since the training phase (and the code) is basically the same.
All the pre-changes done for every specific network have been executed in their respective classes, as well as the data re-normalization, specific for every model.
In fact, ResNet18, ViT and DINOv2 are pre-trained on the same dataset (ImageNet), while CLIP has benn pre-trained using a different dataset (property of openai, in our case, but other models exist).

We also decided to split all the checkpoints saved for every model, but also (and most important) we keep just ONE single file for the best parameters, that we update if,
during an epoch, the new accuracy is better than the previous one. Less memory used, instead of saving every time for every epoch the parameters.
Finally, we added the early-stopping method to prevent overfitting and the jupyter file to show the results of the ResNet-18 training, even if it's gonna be used for comparison between all
the models' results.



15/05
Every model implemented:
- ViT   ->  taken from torchvision, we have decided to use the base form of the model with 16 patches, since a more powerful basic ViT is not needed in our case.
            Retrained with our dataset, the implementation is very similar to the ResNet-18 one;

- CLIP ResNet50/ViT->   taken by open_clip, since the number of images used for its pre-training is quite large, the only part that has been trained is the "dual-head" final part.
                        In other words, the CLIP architecture before the FCs is frozen, such that the backpropagation is done only on the FCs, without touching the original parameters of CLIP.
                        We must test this choice for the network, but, just following the theory, this choice does make sense, considering the consistent pre-training done.
                        Finally, only the image part of the model is used, since the text part is not useful for our purpose (we want to use a final LLM);

- DINOv2 -> base version, taken transformers, even here the DINO architecture is frozen, except for the final FCs. We've followed the same idea made for the CLIP model, cosnidering
            the fact that the dataset used to pre-train this model is very large.

IMPORTANT!!
None of the ViT models has been trained, since the computational power needed is massive. We need to wait for Uni's GPU.